% Encoding: UTF-8

@inproceedings{nairhinton,
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  biburl = {https://www.bibsonomy.org/bibtex/2059683ca9b2457d248942520babbe000/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2010},
  editor = {Fürnkranz, Johannes and Joachims, Thorsten},
  ee = {https://icml.cc/Conferences/2010/papers/432.pdf},
  interhash = {acefcb0a5d1a937232f02f3fe0d5ab86},
  intrahash = {059683ca9b2457d248942520babbe000},
  keywords = {dblp},
  pages = {807-814},
  publisher = {Omnipress},
  timestamp = {2019-04-04T11:48:32.000+0200},
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2010.html#NairH10},
  year = 2010
}

@Misc{flavio,
  author = {Flavio Forenza},
  title  = {Tecniche innovative di Computer Vision per la Foto-Identificazione dei cetacei},
  year   = {????},
  note   = {Tesi di laurea triennale, a.a. 2017/2018, rel. prof. Giovanni Dimauro, correl. dr. Vito Renò},
  school = {Università degli Studi di Bari},
}

@Book{dlbook,
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  note      = {\url{http://www.deeplearningbook.org}},
}

@Misc{cs231n,
  author = {Andrej Karpathy},
  title  = {Lectures of Stanford class CS231n: Convolutional Neural Networks for Visual Recognition},
  year   = {2019},
  note   = {Appunti del corso Stanford CS class C231n.},
}

@Book{Russell2009,
  title     = {Artificial Intelligence: A Modern Approach},
  publisher = {Pearson},
  year      = {2009},
  author    = {Stuart Russell and Peter Norvig},
}

@Article{cifar10,
  author = {Alex Krizhevsky},
  title  = {Learning multiple layers of features from tiny images},
  year   = {2009},
}

@InCollection{alexnet,
  author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {Curran Associates, Inc.},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {1097--1105},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@Article{historydl,
  author      = {Md Zahangir Alom and Tarek M. Taha and Christopher Yakopcic and Stefan Westberg and Paheding Sidike and Mst Shamima Nasrin and Brian C Van Esesn and Abdul A S. Awwal and Vijayan K. Asari},
  title       = {The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches},
  abstract    = {Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].},
  date        = {2018-03-03},
  eprint      = {http://arxiv.org/abs/1803.01164v2},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1803.01164v2:PDF},
  keywords    = {cs.CV},
}

@Misc{imagenet2012,
  title = {ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012)},
  year  = {2012},
  url   = {http://image-net.org/challenges/LSVRC/2012/results},
}

@Book{PCA,
  title     = {Analisi multidimensionale dei dati. Metodi, strategie e criteri d'interpretazione},
  publisher = {Carocci Editore},
  year      = {1999},
  author    = {Sergio Bolasco},
}

@Article{dropout,
  author      = {Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
  title       = {Improving neural networks by preventing co-adaptation of feature detectors},
  year        = {2012},
  abstract    = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  date        = {2012-07-03},
  eprint      = {http://arxiv.org/abs/1207.0580v1},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1207.0580v1:PDF},
  keywords    = {cs.NE, cs.CV, cs.LG},
}

@Comment{jabref-meta: databaseType:bibtex;}
